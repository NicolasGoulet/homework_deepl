{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtS0KIifreXc"
      },
      "source": [
        "# Machine Learning II: Deep Learning and Applications\n",
        "# Homework 2\n",
        "\n",
        "**Due date: Nov 3th, 2025**\n",
        "\n",
        "### Instructions\n",
        "- Make a copy of this notebook in your own Colab and complete the questions there.\n",
        "- You can add more cells if necessary. You may also add descriptions to your code, though it is not mandatory.\n",
        "- Make sure the notebook can run through by *Runtime -> Run all*. **Keep all cell outputs** for grading.\n",
        "- Submit the link of your notebook [here](https://docs.google.com/forms/d/e/1FAIpQLSdEhoIthUqZpgA6WmsS-hUFPZebU4CgtPMMno2Bnm4AduYKcw/viewform?usp=sharing&ouid=108990008229336794809). Please **add TAs as editors** (below) so that you can receive feedback from TAs.\n",
        "  - Click `Share` and add zhihao.zhan@mila.quebec and xinyu.yuan@mila.quebec as editors before your submission.\n",
        "\n",
        "### Note\n",
        "A friendly reminder from the TAs: These exercises are fundamental, so we strongly encourage you to complete them with little to no assistance from ChatGPT, especially if you're pursuing a career as an MLE or applied scientist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q23qmcGYr6AG"
      },
      "source": [
        "# Environment Setup\n",
        "Install necessary python packages for this homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_OZwXO-r5OB",
        "outputId": "614adde6-32d2-4311-cd5a-81e16437de03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.1.4)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1+cu118)\n",
            "  Using cached https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (15.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
            "Installing collected packages: triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0\n",
            "    Uninstalling torch-2.5.0:\n",
            "      Successfully uninstalled torch-2.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.0.1+cu118 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.1+cu118 triton-2.0.0\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install datasets transformers\n",
        "!pip install tiktoken\n",
        "!pip install omegaconf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7a-_yxpWV2T",
        "outputId": "dfa63d56-2bb2-41f5-e47f-16158b4a766e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A29mp7wsC_I1"
      },
      "source": [
        "Now let's download all the files needed using the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydUVO0yBDCO_",
        "outputId": "c6e5f00b-cfc6-4fbc-95f5-7c61299ecc95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17XsqhDy_GCjJex7Ekg6PKYEZUKkL_iqH\n",
            "To: /home/alkan/homework_deepl/english-tokenizer.json\n",
            "100%|██████████████████████████████████████| 32.7k/32.7k [00:00<00:00, 2.29MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1GSTq3NQO519BkEG9Bid7003Awh17YrXx\n",
            "From (redirected): https://drive.google.com/uc?id=1GSTq3NQO519BkEG9Bid7003Awh17YrXx&confirm=t&uuid=637ada33-36c3-440f-94b5-08e6497d808f\n",
            "To: /home/alkan/homework_deepl/fixed_initialized_model.pt\n",
            "100%|████████████████████████████████████████| 116M/116M [00:14<00:00, 8.11MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=18LcbPdiyaWAdnBUfwSSx6lmzVE0cSWrP\n",
            "From (redirected): https://drive.google.com/uc?id=18LcbPdiyaWAdnBUfwSSx6lmzVE0cSWrP&confirm=t&uuid=444d1a67-3a6c-4982-a030-86a6a56598ea\n",
            "To: /home/alkan/homework_deepl/tokens.npz\n",
            "100%|████████████████████████████████████████| 570M/570M [01:17<00:00, 7.38MB/s]\n"
          ]
        }
      ],
      "source": [
        "! gdown \"https://drive.google.com/uc?id=17XsqhDy_GCjJex7Ekg6PKYEZUKkL_iqH\"\n",
        "! gdown \"https://drive.google.com/uc?id=1GSTq3NQO519BkEG9Bid7003Awh17YrXx\"\n",
        "! gdown \"https://drive.google.com/uc?id=18LcbPdiyaWAdnBUfwSSx6lmzVE0cSWrP\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzvp3rx9sMvj"
      },
      "source": [
        "# Task 1: Transformer pre-training pipeline using HuggingFace library\n",
        "\n",
        "\n",
        " In this task, you will develop a basic Transformer model and explore training processes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmE0HY0gLwzV"
      },
      "source": [
        "We first compile some utilization functions.\n",
        "\n",
        "**Note: this part of function has to be compiled to run following section. Do not modify the seed to avoid incorrect evaluation results.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7Z7Ig6J4LwM-"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------------ #\n",
        "###############  Utilization Functions (DO NOT MODIFY) ###############\n",
        "\n",
        "import torch\n",
        "import io\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def determine_device() -> str:\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    return \"cpu\"\n",
        "\n",
        "\n",
        "def estimate_model_disk_size(model: torch.nn.Module) -> int:\n",
        "    with io.BytesIO() as byte_stream:\n",
        "        torch.save(model.state_dict(), byte_stream)\n",
        "        return byte_stream.tell()\n",
        "\n",
        "def count_params(model: torch.nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "def enable_tf32() -> None:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "def set_seed(seed):\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "# ------------------------------------------------------------------------------------ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsia4UuvsgTS"
      },
      "source": [
        "## Transformer Implementation\n",
        "\n",
        "In this section, you will implement the key components of the Transformer architecture, specifically the Transformer decoder as depicted in the following Figure 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovy3hlz6pqQ8"
      },
      "source": [
        "![image](https://drive.google.com/uc?id=1Ia7d-1_hdk31E9BV-4_3PYgmNX3YgbM5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvVEk3Z4oWB4"
      },
      "source": [
        "You will begin by creating a decoder-only transformer model, following the provided code structure in the following `Model` section 1 and 2. This structure includes all necessary classes and function declarations required for your submission. Specifically, code to implement is occupied with \"...\" with clear commented note \"[TODO]\", and is detailed in the following subquestions. Please,\n",
        "\n",
        "1. **avoid altering these elements or adding new Python dependencies** to ensure compatability with the automated testing pipeline, which cold otherwise lead to test failures.\n",
        "\n",
        "2. **aim for a model that is efficiently implemented**, favoring the use of PyTorch functions and avoiding loop-based matrix operations\n",
        "\n",
        "3. **You should not use overly simplistic layers or functions** like torch.nn.TransformerDecoder.\n",
        "\n",
        "If in doubt about the appropriateness of a method, consult with the course TAs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sZcEKjnebrf6"
      },
      "outputs": [],
      "source": [
        "############ Model Section 1 (DO NOT MODIFY) ############\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Dimension symbols:\n",
        "    B - batch size\n",
        "    S - sequence length\n",
        "    D - hidden dimension (n_embd)\n",
        "    H - number of attention heads (n_head)\n",
        "    HD - hidden dimension of a single attention head (d // n_head)\n",
        "    V - size of the vocabulary\n",
        "\"\"\"\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, n_embd, head_size, dropout, n_positions):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(n_positions, n_positions)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        #Note: this dropout randomly prevents some tokens from communicating with each other\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x) #shape (B,T, head_size)\n",
        "        q = self.query(x) #shape (B,T, head_size)\n",
        "        v = self.value(x) #shape (B,T, head_size)\n",
        "\n",
        "        #compute self-attention scores\n",
        "        wei = q @ k.transpose(-2, -1) #shape (B,T, head_size) @ (B,head_size,T) --> (B,T,T)\n",
        "        wei *= C**-0.5 #scale by sqrt(d_k) as per paper, so that variance of the wei is 1\n",
        "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B,T,T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        #perform weighted aggregation of values\n",
        "        out = wei @ v # (B, T, T) @ (B, T, head_size) --> (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-head attention as a collection of heads with concatenated outputs.\"\"\"\n",
        "    def __init__(self, n_embd, n_head, dropout, n_positions):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.heads = nn.ModuleList([Head(n_embd, head_size, dropout, n_positions) for _ in range(n_head)])\n",
        "        self.proj  = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" the feed forward network (FFN) in the paper\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd*4, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"A single decoder block in a decoder language model.\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, dropout, n_positions):\n",
        "        \"\"\"Initialize the modules used in a decoder block.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadAttention(n_embd, n_head, dropout, n_positions)\n",
        "        self.mlp = FeedForward(n_embd, dropout)\n",
        "        self.ln_2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.FloatTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "        x = self.ln_1(x + self.attn(x))\n",
        "        x = self.ln_2(x + self.mlp(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kn7QRwvQDM8_"
      },
      "outputs": [],
      "source": [
        "############ Model Section 2 ############\n",
        "\n",
        "class DecoderLM(nn.Module):\n",
        "    \"\"\"The decoder language model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab: int,\n",
        "        n_embd: int,\n",
        "        n_head: int,\n",
        "        n_positions: int,\n",
        "        n_layer: int,\n",
        "        p_dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_vocab = n_vocab\n",
        "        self.n_embd = n_embd\n",
        "        self.n_head = n_head\n",
        "        self.n_positions = n_positions\n",
        "        self.n_layer = n_layer\n",
        "        self.p_dropout = p_dropout\n",
        "\n",
        "        self.token_embeddings = nn.Embedding(n_vocab, n_embd)\n",
        "        self.position_embeddings = nn.Embedding(n_positions, n_embd)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [DecoderBlock(n_embd=n_embd, n_head=n_head, dropout=p_dropout, n_positions=n_positions) for _ in range(n_layer)]\n",
        "        )\n",
        "        self.lm_head = nn.Linear(self.n_embd, self.n_vocab, bias=False)\n",
        "        # NOTE: layer_norm should be put after transformer blocks `self.blocks`,\n",
        "        # and before the language model head `self.lm_head`\n",
        "        self.ln = nn.LayerNorm(n_embd)\n",
        "        self.dropout = nn.Dropout(self.p_dropout)\n",
        "\n",
        "        # initialize weights according to nanoGPT\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith(\"out_proj.weight\"):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / torch.sqrt(torch.tensor(2 * n_layer)))\n",
        "\n",
        "        # tie the output projection weights to the token embedding weights\n",
        "        self.lm_head.weight = self.token_embeddings.weight\n",
        "\n",
        "        # count flops per token\n",
        "        self.flops_per_token = (\n",
        "            6 * count_params(self) + 12 * n_layer * n_embd * n_positions\n",
        "        )\n",
        "\n",
        "    def embed(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"Convert input_ids to embeddings (token_embeddings + positional_embeddings).\n",
        "\n",
        "        Args:\n",
        "            input_ids: tokens ids with shape (B x S)\n",
        "\n",
        "        Returns:\n",
        "            embeddings: token representations with shape (B x S x D)\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Position ids are indices of tokens in the sequence. They are simply [0, 1, 2, ...] for every sequence in the\n",
        "        batch.\n",
        "\n",
        "        Example (B = 2, S = 5):\n",
        "\n",
        "        position_ids = tensor([\n",
        "         [0, 1, 2, 3, 4],\n",
        "         [0, 1, 2, 3, 4]\n",
        "        ])\n",
        "        \"\"\"\n",
        "\n",
        "        assert input_ids.shape[1] <= self.n_positions\n",
        "        # B = batch_size, S = seq_len\n",
        "        B, S = input_ids.shape\n",
        "\n",
        "        # token embeddings: (B, S, D)\n",
        "        token_embeddings = self.token_embeddings(input_ids)\n",
        "\n",
        "        # position ids: (1, S) -> (B, S)\n",
        "        position_ids = torch.arange(S, device=input_ids.device).unsqueeze(0).expand(B, S)\n",
        "\n",
        "        # positional embeddings: (B, S, D)\n",
        "        positional_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        # sum + dropout\n",
        "        return self.dropout(token_embeddings + positional_embeddings)\n",
        "\n",
        "    def token_logits(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"Project the final hidden states of the model to token logits.\n",
        "\n",
        "        Args:\n",
        "            x: hidden states produced by the final decoder block (B x S x D)\n",
        "\n",
        "        Returns:\n",
        "            logits: logits corresponding to the predicted next token likelihoods (B x S x V)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"A forward pass of the decoder LM, converting input_ids to token logits.\n",
        "\n",
        "        Args:\n",
        "            input_ids: tokens ids with shape (B x S)\n",
        "\n",
        "        Returns:\n",
        "            logits: logits corresponding to the predicted next token likelihoods (B x S x V)\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"A forward pass of the decoder LM, converting input_ids to token logits.\"\"\"\n",
        "        # 1) embed tokens + positions\n",
        "        x = self.embed(input_ids)          # (B, S, D)\n",
        "\n",
        "        # 2) pass through decoder blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)                   # (B, S, D)\n",
        "\n",
        "        # 3) final layer norm\n",
        "        x = self.ln(x)                     # (B, S, D)\n",
        "\n",
        "        # 4) project to vocab\n",
        "        logits = self.token_logits(x)      # (B, S, V)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "414k1aqv_33C"
      },
      "source": [
        "$\\textbf{Question 1.1}$ (5 points) **Weight Tying**\n",
        "\n",
        "[Press and Wolf (2017)](https://aclanthology.org/E17-2025/) proposed a weight tying technique for projecting hidden states of a language model to token logits.\n",
        "\n",
        "**Read** this paper first.\n",
        "\n",
        "**Write code** in `DecoderLM.__init__()` to implement the weight Tying technique\n",
        "\n",
        "**In your report**, explain what weight tying does.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER TO QUESTION 1.1**\n",
        "\n",
        "The general idea is to have the mdel use the same matrix to read and predict words. \n",
        "\n",
        "In models like the one considered here, we usually have two big vocab metrics of different dimensions, i.e. the input embedding matrix (which turns tokens into vectors) and the output projection (which turns hidden vectors into logits over vocab). This results in two seperate learnable matrices. \n",
        "\n",
        "Weight tying forces those two matrices to be the same tensor, resulting in one parameter matrix in memory, with both layers pointing to it. This has the following double-advantage : (i) as the model learns better embeddings, it improves at the same time the output head ; (ii) similarly, when the model learns better output predictions, it also improves the embeddings. \n",
        "\n",
        "This is possible due to a key insight that both operations are two side of the same coins : *given a token i, what vector represents it?\" VS \"given a vector, how likely is token i?\". It therefore makes sense to have them share parameters. \n",
        "\n",
        "This resulted in cutting the number of parameters in half WHILE increasing performance. Something that we have seen recently with the Tiny Models by Alexia Jolicoeur-Martineau who achieved better results than gemini models with 0.01% of the parameters and an under 500USD budget. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfLwx9Iwm8dO"
      },
      "source": [
        "$\\textbf{Question 1.2}$ (21 points) **Transformer Model**\n",
        "\n",
        "\n",
        "Recall the Transformer Decoder shown in Figure 1. You are expected\n",
        "to **implement the `DecoderLM` class** for the full decoder model:\n",
        "\n",
        "   - the embedding step (6 points): implement the `DecoderLM.embed()` function, to convert input token indices into token embeddings, combine with positional embeddings to create embeddings for transformer input.\n",
        "   - the final output logits (3 points): implement the `DecoderLM.token_logits()` function, to project the final hidden states of the model to token logits.\n",
        "   - decoder blocks (12 points): implement the `DecoderLM.forward()` function by calling `DecoderLM.embed()`, passing the resulted embeddings to transformer blocks as input, get the final hidden states from the transformer blocks, and calling `DecoderLM.token_logits()` to get the final token logits.\n",
        "\n",
        "Now please read `Model` section 1 carefully and complete the `Model` section 2 above, by implementing all the \"[TODO]\" specified. Your implementation will earn points for each of the \"[TODO]\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BZukjtqslAe"
      },
      "source": [
        "## Transformer Pre-training\n",
        "\n",
        "Now that you've set up the transformer, it's time to begin training the model! For ease of implementation and testing, the tokenized input will be provided to you. We use a portion of the C4 corpus, a refined subset of the Common Crawl web corpus. The dataset is automatically downloaded as a part of the training script, eliminaring the need for you to manually access it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbSVJkHE2_S6"
      },
      "source": [
        "$\\textbf{Question 2.1}$ (24 points) **Model Training Pipeline**\n",
        "\n",
        "The training pipeline is outlined in the `Model Training` section 1, 2, 3, and 4 below. Now please read `Model Training` section 1 and 4 carefully and complete the `Model Training` section 2 and 3 below, by implementing all the \"[TODO]\" specified. Specifically, you are expected to **implement three key functions**:\n",
        "\n",
        "- cosine_lr_schedule (6 points) - a learning rate scheduler that uses Cosine annealing (refer to question 3.2).\n",
        "- compute_language_modeling_loss (6 points) - the function to calculate loss for both training and evaluating the model.\n",
        "- train (12 points) - the main loop for training the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayRzdP7v1kYj"
      },
      "source": [
        "We use [wandb](https://wandb.ai/) for logging the training curves for visualization. you will need to have a [weights & biases account](https://wandb.ai/login)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VoYsADP1lLN",
        "outputId": "9d689541-3dc8-48be-dbc9-52344713c6c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "Aborted!\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! wandb login --relogin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/alkan/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnicolas-goulet\u001b[0m (\u001b[33mnicolas-goulet-hec\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"3fffd4c6035fdc6b19fb004e953a0a4281ffdcb8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfWR5wnCFDB3"
      },
      "source": [
        "**Remember** to consider various training hyperparameters such as batch size, learning rate (including its scheduler), and gradient accumulation, among others. These parameters are set in a configuration dictionary `hyperparam_config` in the `Training Hyper-parameter` section, and we have provided sample configurations for you to adjust these settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YH0Ema1FKLln"
      },
      "outputs": [],
      "source": [
        "############ Training Hyper-parameter Section ############\n",
        "hyperparam_config = {\n",
        "    \"output_dir\": \"outputs/GPT-4060ti-320d\",\n",
        "    \"tokenizer_encoding\": \"gpt2\",\n",
        "    \"model_config\": {\n",
        "        \"n_embd\": 288,      # bigger model\n",
        "        \"n_head\": 8,\n",
        "        \"n_positions\": 256, # longer context → more VRAM\n",
        "        \"n_layer\": 6,\n",
        "    },\n",
        "    \"device\": \"auto\",\n",
        "    \"batch_size\": 48,       \n",
        "    \"seq_len\": 256,\n",
        "    \"num_warmup_steps\": 100,\n",
        "    \"num_training_steps\": 10000,\n",
        "    \"grad_accumulation_steps\": 4,\n",
        "    \"min_lr\": 3e-5,\n",
        "    \"max_lr\": 3e-4,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YwHlkUX6Dbup"
      },
      "outputs": [],
      "source": [
        "############ Model Training Section 1 (DO NOT MODIFY) ############\n",
        "\n",
        "\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from collections import deque\n",
        "from collections.abc import Iterator\n",
        "from contextlib import nullcontext\n",
        "from typing import Callable\n",
        "from rich import print\n",
        "\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "from einops import rearrange\n",
        "from omegaconf import OmegaConf\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "\n",
        "def random_batch_sampler(\n",
        "    tokens: torch.LongTensor, device: str, batch_size: int, seq_len: int\n",
        ") -> Iterator[torch.LongTensor]:\n",
        "    \"\"\"An infinite generator that samples batches of sequences from the tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens: a 1d torch tensor of token ids\n",
        "        device: the device to put the batch on\n",
        "        batch_size: the batch size of the output tensor (B)\n",
        "        seq_len: the sequence length of the output tensor (S)\n",
        "\n",
        "    Returns:\n",
        "        An infinite generator that samples batches of sequences from the\n",
        "        tokens. Each batch has shape (B x S). Every sequence in the batch is\n",
        "        a contiguous subsequence of tokens, sampled uniformly at random. The\n",
        "        output tensor should be on the right device.\n",
        "    \"\"\"\n",
        "\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    while True:\n",
        "        # Generate random starting indices for each sequence in the batch\n",
        "        indices = np.random.randint(0, num_tokens - seq_len + 1, batch_size)\n",
        "\n",
        "        # Gather sequences from the tokens\n",
        "        batch = torch.stack([tokens[i:i + seq_len] for i in indices])\n",
        "\n",
        "        # Move the batch to the specified device\n",
        "        yield batch.to(device)\n",
        "\n",
        "\n",
        "def sequential_batch_sampler(\n",
        "    tokens: torch.LongTensor, device: str, batch_size: int, seq_len: int\n",
        ") -> Iterator[torch.LongTensor]:\n",
        "    \"\"\"A generator that yields batches of tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens: a 1d torch tensor of token ids\n",
        "        device: the device to put the batch on\n",
        "        batch_size: the batch size of the output tensor (B)\n",
        "        seq_len: the sequence length of the output tensor (S)\n",
        "\n",
        "    Returns:\n",
        "        A generator that yields a batch of tokens at a time. Each batch has\n",
        "        shape (B x S). Every sequence in the batch is a contiguous subsequence\n",
        "        of tokens in sequential order. The output tensor should be on the right\n",
        "        device.\n",
        "\n",
        "    Note: If the last batch is incomplete, which could happen when the number\n",
        "        of tokens is not divisible by (batch_size * seq_len), you could drop\n",
        "        the last batch.\n",
        "    \"\"\"\n",
        "\n",
        "    num_tokens = len(tokens)\n",
        "    total_batches = num_tokens // (batch_size * seq_len)\n",
        "\n",
        "    for batch_idx in range(total_batches):\n",
        "        start_idx = batch_idx * batch_size * seq_len\n",
        "        end_idx = start_idx + batch_size * seq_len\n",
        "\n",
        "        # Extract the batch of tokens\n",
        "        batch = tokens[start_idx: end_idx].view(batch_size, seq_len)\n",
        "\n",
        "        # Move the batch to the specified device and yield\n",
        "        yield batch.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yz01X_AHhOIs"
      },
      "outputs": [],
      "source": [
        "############ Model Training Section 2 ############\n",
        "\n",
        "def cosine_lr_schedule(\n",
        "    num_warmup_steps: int,\n",
        "    num_training_steps: int,\n",
        "    min_lr: float,\n",
        "    max_lr: float,\n",
        ") -> Callable[[int], float]:\n",
        "    def get_lr(t: int) -> float:\n",
        "        \"\"\"Outputs the learning rate at step t under the cosine schedule.\n",
        "\n",
        "        Args:\n",
        "            t: the current step number\n",
        "\n",
        "        Returns:\n",
        "            lr: learning rate at step t\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        assert max_lr >= min_lr >= 0.0\n",
        "        assert num_training_steps >= num_warmup_steps >= 0\n",
        "\n",
        "        if t <= num_warmup_steps:\n",
        "            # Linear warmup starting from 0 to max_lr\n",
        "            warmup_steps = max(1, num_warmup_steps)\n",
        "            lr = max_lr * (t / warmup_steps)\n",
        "        elif t >= num_training_steps:\n",
        "            # After training steps, return min_lr\n",
        "            lr = min_lr\n",
        "        else:\n",
        "            # Cosine decay from max_lr to min_lr\n",
        "            # progress goes from 0 → 1 over the decay phase\n",
        "            progress = (t - num_warmup_steps) / (num_training_steps - num_warmup_steps)\n",
        "            cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "            lr = min_lr + (max_lr - min_lr) * cosine\n",
        "        return lr\n",
        "\n",
        "    return get_lr\n",
        "\n",
        "\n",
        "def set_lr(optimizer: torch.optim.Optimizer, lr: float) -> None:\n",
        "    for g in optimizer.param_groups:\n",
        "        g[\"lr\"] = lr\n",
        "\n",
        "\n",
        "\n",
        "def compute_language_modeling_loss(\n",
        "    input_ids: torch.LongTensor, logits: torch.FloatTensor\n",
        ") -> torch.FloatTensor:\n",
        "    \"\"\"Outputs the language modeling loss given input_ids and logits\n",
        "\n",
        "    Args:\n",
        "        input_ids: the input token ids (B, S)\n",
        "        logits: the next token logits produced by the language model (B, S, V)\n",
        "\n",
        "    Returns:\n",
        "        loss: the mean cross entropy loss for next token prediction\n",
        "    \"\"\"\n",
        "    # shift inputs to get labels: each position predicts the NEXT token\n",
        "    # labels: (B, S-1)\n",
        "    labels = input_ids[:, 1:]\n",
        "\n",
        "    # drop the last time step in logits so shapes match: (B, S-1, V)\n",
        "    logits = logits[:, :-1, :]\n",
        "\n",
        "    # flatten for cross-entropy: logits -> (B*(S-1), V), labels -> (B*(S-1),)\n",
        "    B, S_minus_1, V = logits.shape\n",
        "    logits_flat = logits.reshape(B * S_minus_1, V)\n",
        "    labels_flat = labels.reshape(B * S_minus_1)\n",
        "\n",
        "    loss = F.cross_entropy(logits_flat, labels_flat)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7e7Yong_0W-5"
      },
      "outputs": [],
      "source": [
        "############ Model Training Section 3 ############\n",
        "\n",
        "def train(\n",
        "    model: DecoderLM,\n",
        "    batch_sampler: Iterator[torch.LongTensor],\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    lr_schedule: Callable[[int], float],\n",
        "    autocast: torch.autocast | nullcontext = nullcontext(),\n",
        "    num_training_steps: int = 0,\n",
        "    grad_accumulation_steps: int = 1,\n",
        ") -> None:\n",
        "    \"\"\"A training loop for the language model\n",
        "\n",
        "    Args:\n",
        "        model: the decoder LM\n",
        "        batch_sampler: a generator that produces batches of token ids\n",
        "        optimizer: an optimizer for gradient update\n",
        "        lr_schedule: a callable that produces the learning at a step number\n",
        "        autocast: a context manager that handles tensor casting (you do not need\n",
        "          to care about this for your implementation)\n",
        "        num_training_steps: number of steps to train for\n",
        "        grad_accumulation_steps: number of \"micro\" training steps before each\n",
        "          gradient update\n",
        "    \"\"\"\n",
        "    # stores training losses for the 20 latest steps\n",
        "    losses = deque(maxlen=20 * grad_accumulation_steps)\n",
        "\n",
        "    for step in (pbar := trange(num_training_steps)):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # 1) set LR for this step\n",
        "        lr = lr_schedule(step)\n",
        "        set_lr(optimizer, lr)\n",
        "\n",
        "        # 2) accumulate gradients over micro-steps\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        for _ in range(grad_accumulation_steps):\n",
        "            input_ids = next(batch_sampler)\n",
        "            with autocast:\n",
        "                logits = model(input_ids)\n",
        "            loss = compute_language_modeling_loss(input_ids, logits)\n",
        "            # scale loss so total gradient over accumulation is same as single step\n",
        "            (loss / grad_accumulation_steps).backward()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # 3) update params\n",
        "        optimizer.step()\n",
        "\n",
        "        # logging stuff\n",
        "        loss_mean = np.mean(losses).item()\n",
        "\n",
        "        FLOPs_per_step = (\n",
        "            model.flops_per_token\n",
        "            * input_ids.shape[0]\n",
        "            * input_ids.shape[1]\n",
        "            * grad_accumulation_steps\n",
        "        )\n",
        "        t1 = time.time()\n",
        "        dt = t1 - t0\n",
        "        t0 = t1\n",
        "        pbar.set_postfix(\n",
        "            {\n",
        "                \"train loss\": f\"{loss_mean:.2f}\",\n",
        "                \"TFLOPS\": f\"{FLOPs_per_step / dt / 1e12:.1f}\",\n",
        "            }\n",
        "        )\n",
        "        wandb.log({\"train-loss\": loss_mean, \"learning-rate\": lr}, step=step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgi3T6IhCdEA"
      },
      "source": [
        "$\\textbf{Question 2.2}$ (10 points) **Model Training Results**\n",
        "\n",
        "**Execute the `Model Training` Section 4** below to run the pre-training pipeline, and report the training loss curve. (Note: runnning for 60 steps takes around **8 minutes** for training.)\n",
        "\n",
        "\n",
        "**In your report**, answer the following:\n",
        "\n",
        "A. Plot the training curve for the default `hyperparam_config` (use a screenshot from weights & biases: https://wandb.ai/site), and discuss how and why it looks like that.\n",
        "\n",
        "B. Discuss your `DecoderLM` pre-training experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![cat](small_3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER QUESTION 2.2**\n",
        "\n",
        "A. WHY IT LOOKS LIKE THAT\n",
        "\n",
        "The first part of this plot (steps 0 to 100) shows the **warmup** phase. It is an initial phase where the learning rate (LR) is increased from 0 to the maximum value desired of the LR. At the beginning weights are random, activations / layernorm not settled yet, Adam's *m* and *v* are still at 0 ; these factors, to which we can add batch size, suggest a very large gradient. A big LR would therefore be too sensitive to the gradient and result in a very noisy training. The warmup phase therefore *smoothly guides* the model *down the valley* until the point where we can have a better *guide*, cosine decay. Once the warmup is done, the training gets much more noisy since the task gets harder : the model has already learned easy / high-frequency patterns, like punctuations and function words in the warmup phase, and has now to start learning more *complex stuff* about language, like rarer tokens or more context-dependant tokens. Once the the model reaches a certain plateau (due to its size), there seems to be an ongoing (but very slight) trend toward a smaller and smaller training loss, while the noisiness due to the randomness of the sampling takes more and more place (since we are taking random batches of 520M tokens), hinting that there is no much more left for progress. This was confirmed by asking a colleague of mine what is training loss and number of steps were : he reached around 4.8 loss with 8k steps (!), but I do not know the other differences for our models.\n",
        "\n",
        "\n",
        "\n",
        "B. DISCUSS DECODERLM PRETRAINING EXPERIMENT\n",
        "\n",
        "The first thing that needs to be addressed is the limitations inherent to using GradeScope, as we had to restrein ourselves to a model of 100mb. \n",
        "\n",
        "Once that the ceiling was set, the *goal of the game* was reaching the smallest loss possible while staying under the model size limit. I have to admit my approach was not very creative : one run with the starting hyperparams, one with slightly bigger, then an aborted one because I was going over 100mb, and a final one that was around 94.8mb. \n",
        "\n",
        "In that sense, the only way to do something that involves more critical thinking would be : coding a tuner that searches the optimal hyperparams considering the 100MB limit, keep trying until by hand and inductions for trials and errors until I find the best model within 100mb (because tuners, with REALLY big model, are just not feasible due to the cost (sometimes)), or playing with the code we are not supposed to, by reading papers on the state-of-the-art of training methods for such models. \n",
        "\n",
        "**ADDENDUM** After going through each of the possible hyperparam (I was too much focused on stacking lego blocks by simply increasing the size of the model), I realized I could increase the number of `grad_accumulation_steps` without changing the size of the model, moving to value from 1 to 4. This means my model will do 4 forward and backward pass on each batch before calling the optimizer, using more VRAM and taking longer but hopefully improving my loss. I also read about how bigger models are trained, and it is common to see 50k to 200k steps trained model. I therefore also increased the number of steps to 10k. It is hard for me to actually interpret what this means, I would have had to test the models on various task, and see how its performance varied across different cycles of pre-training / fine-tuning / perform a task. \n",
        "\n",
        "As we can see in the resulting plot, we indeed reach a 50% improvement for training loss (going from around 6 to around 4, ending at 3.95). However, starting from step 6000, the improvement rate significantly decreases (as shown more clearly direclty on the WandB web GUI), with barely any gains for the remainder of the pre-training. To me, this confirms the intuitions of my previous paragraph, written before executing this training.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![cat](big_attempt.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "M0iMgp89hQWb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alkan/VirtualEnv/homeWorkEnv/lib/python3.10/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">########################################\n",
              "output_dir: outputs/GPT-4060ti-320d\n",
              "tokenizer_encoding: gpt2\n",
              "model_config:\n",
              "  n_embd: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">288</span>\n",
              "  n_head: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>\n",
              "  n_positions: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>\n",
              "  n_layer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>\n",
              "device: auto\n",
              "batch_size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48</span>\n",
              "seq_len: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>\n",
              "num_warmup_steps: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>\n",
              "num_training_steps: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span>\n",
              "grad_accumulation_steps: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
              "min_lr: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0e-05</span>\n",
              "max_lr: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0003</span>\n",
              "########################################\n",
              "</pre>\n"
            ],
            "text/plain": [
              "########################################\n",
              "output_dir: outputs/GPT-4060ti-320d\n",
              "tokenizer_encoding: gpt2\n",
              "model_config:\n",
              "  n_embd: \u001b[1;36m288\u001b[0m\n",
              "  n_head: \u001b[1;36m8\u001b[0m\n",
              "  n_positions: \u001b[1;36m256\u001b[0m\n",
              "  n_layer: \u001b[1;36m6\u001b[0m\n",
              "device: auto\n",
              "batch_size: \u001b[1;36m48\u001b[0m\n",
              "seq_len: \u001b[1;36m256\u001b[0m\n",
              "num_warmup_steps: \u001b[1;36m100\u001b[0m\n",
              "num_training_steps: \u001b[1;36m10000\u001b[0m\n",
              "grad_accumulation_steps: \u001b[1;36m4\u001b[0m\n",
              "min_lr: \u001b[1;36m3.0e-05\u001b[0m\n",
              "max_lr: \u001b[1;36m0.0003\u001b[0m\n",
              "########################################\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/alkan/homework_deepl/wandb/run-20251031_120942-5xcm37ih</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nicolas-goulet-hec/llms-hw2/runs/5xcm37ih' target=\"_blank\">murky-fog-5</a></strong> to <a href='https://wandb.ai/nicolas-goulet-hec/llms-hw2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nicolas-goulet-hec/llms-hw2' target=\"_blank\">https://wandb.ai/nicolas-goulet-hec/llms-hw2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nicolas-goulet-hec/llms-hw2/runs/5xcm37ih' target=\"_blank\">https://wandb.ai/nicolas-goulet-hec/llms-hw2/runs/5xcm37ih</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model parameters = 21M\n",
              "</pre>\n"
            ],
            "text/plain": [
              "model parameters = 21M\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your model is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">94.</span>8MB. This should be within the 100MB limit of Gradescope.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Your model is \u001b[1;36m94.\u001b[0m8MB. This should be within the 100MB limit of Gradescope.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">train dataset tokens = 520M\n",
              "</pre>\n"
            ],
            "text/plain": [
              "train dataset tokens = 520M\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">train FLOPs = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.32e+16</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "train FLOPs = \u001b[1;36m6.32e+16\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [2:14:10<00:00,  1.24it/s, train loss=3.96, TFLOPS=7.9] \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model saved to outputs/GPT-4060ti-320d/model.pt\n",
              "</pre>\n"
            ],
            "text/plain": [
              "model saved to outputs/GPT-4060ti-320d/model.pt\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "############ Model Training Section 4 (DO NOT MODIFY) ############\n",
        "\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate(\n",
        "    model: DecoderLM,\n",
        "    batch_sampler: Iterator[torch.LongTensor],\n",
        "    autocast: torch.autocast | nullcontext = nullcontext(),\n",
        ") -> dict[str, float]:\n",
        "    losses = []\n",
        "\n",
        "    for input_ids in tqdm(batch_sampler, desc=\"evaluating..\"):\n",
        "        with autocast:\n",
        "            logits = model(input_ids)\n",
        "        loss = compute_language_modeling_loss(input_ids, logits)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # mean of the losses is the average negative log likelihood\n",
        "    mean_loss = sum(losses) / len(losses)\n",
        "    perplexity = math.exp(mean_loss)\n",
        "\n",
        "    eval_results = {\n",
        "        \"val-loss\": mean_loss,\n",
        "        \"val-perplexity\": perplexity,\n",
        "    }\n",
        "    wandb.log(eval_results)\n",
        "    return eval_results\n",
        "\n",
        "def main():\n",
        "    enable_tf32()\n",
        "\n",
        "    # create an output directory and dump the configuration file\n",
        "    config = OmegaConf.create(hyperparam_config)\n",
        "    os.makedirs(config.output_dir, exist_ok=True)\n",
        "    OmegaConf.save(config, os.path.join(config.output_dir, \"config.yaml\"))\n",
        "    print(\"#\" * 40, OmegaConf.to_yaml(config).strip(), \"#\" * 40, sep=\"\\n\")\n",
        "    wandb.init(project=\"llms-hw2\", config=OmegaConf.to_container(config))\n",
        "\n",
        "    # initialize tokenizer and model\n",
        "    tokenizer = tiktoken.get_encoding(config.tokenizer_encoding)\n",
        "    device = determine_device() if config.device == \"auto\" else config.device\n",
        "    model = DecoderLM(tokenizer.n_vocab, **config.model_config).to(device)\n",
        "    print(f\"model parameters = {count_params(model) / 1e6:.0f}M\")\n",
        "\n",
        "    model_disk_size_MB = estimate_model_disk_size(model) * 1e-6\n",
        "    if model_disk_size_MB > 98:\n",
        "        print(\n",
        "            f\"[red]WARNING: your model is {model_disk_size_MB:.1f}MB. \"\n",
        "            \"The largest model size allowed by GradeScope is 100MB, \"\n",
        "            \"and you may have trouble with submitting the assignment. \"\n",
        "            \"Please update your config so your model is at most 100 MB.[/red]\"\n",
        "        )\n",
        "    else:\n",
        "        print(\n",
        "            f\"Your model is {model_disk_size_MB:.1f}MB. This should be within \"\n",
        "            \"the 100MB limit of Gradescope.\"\n",
        "        )\n",
        "\n",
        "    # prepare data and data generator\n",
        "    assert config.seq_len <= config.model_config.n_positions\n",
        "    tokens = np.load(\"tokens.npz\")\n",
        "\n",
        "    train_tokens = torch.from_numpy(tokens[\"train\"].astype(int))\n",
        "    val_tokens = torch.from_numpy(tokens[\"val\"].astype(int))\n",
        "\n",
        "    train_sampler = random_batch_sampler(\n",
        "        train_tokens, device, config.batch_size, config.seq_len\n",
        "    )\n",
        "    val_sampler = sequential_batch_sampler(\n",
        "        val_tokens, device, config.batch_size, config.seq_len\n",
        "    )\n",
        "    print(f\"train dataset tokens = {len(train_tokens) / 1e6:.0f}M\")\n",
        "    FLOPs = (\n",
        "        model.flops_per_token\n",
        "        * config.num_training_steps\n",
        "        * config.grad_accumulation_steps\n",
        "        * config.batch_size\n",
        "        * config.seq_len\n",
        "    )\n",
        "    print(f\"train FLOPs = {FLOPs:.2e}\")\n",
        "    if FLOPs > 1e17:\n",
        "        print(\n",
        "            f\"[red]WARNING: your train FLOPs is {FLOPs:.2e}. \"\n",
        "            \"This is more than the max compute that we allow (1e+17). \"\n",
        "            \"Please reduce your model size or train steps.[/red]\"\n",
        "        )\n",
        "\n",
        "    # prepare optimizer and lr schedule\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=0.0,  # will set this dynamically in the training loop\n",
        "        betas=(0.9, 0.95),\n",
        "        fused=device == \"cuda\",\n",
        "    )\n",
        "    lr_schedule = cosine_lr_schedule(\n",
        "        config.num_warmup_steps, config.num_training_steps, config.min_lr, config.max_lr\n",
        "    )\n",
        "    autocast = (\n",
        "        torch.autocast(\n",
        "            device,\n",
        "            dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32),\n",
        "        )\n",
        "        if device == \"cuda\"\n",
        "        else nullcontext()\n",
        "    )\n",
        "    # training\n",
        "    model.train()\n",
        "    train(\n",
        "        model,\n",
        "        train_sampler,\n",
        "        optimizer,\n",
        "        lr_schedule,\n",
        "        autocast,\n",
        "        config.num_training_steps,\n",
        "        config.grad_accumulation_steps,\n",
        "    )\n",
        "\n",
        "    # save the trained model\n",
        "    model_path = os.path.join(config.output_dir, \"model.pt\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"model saved to {model_path}\")\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijz5fOtt3nb0"
      },
      "source": [
        "# Task 2: Bias in Large Language Models (LLMs)\n",
        "\n",
        "In this task, you will critically examine limitations and bias of LLMs. You will use a pre-trained BERT model in the following `Bias of BERT` section to analyze the potential sources of bias from the data to the model itself.\n",
        "\n",
        "You need to enter the prompts in this notebook, and **in your discussion**, these prompts need to be detailed and explaned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJUxr5sQWgJu"
      },
      "source": [
        "**NOTE**: Please restart the colab runtime before running codes for Task 2 to avoid package dependency issue. **Go to \"Runtime menu\" and click \"Restart Runtime\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4EOE8jUZ-c7"
      },
      "outputs": [],
      "source": [
        "! pip install -qqq torch==2.5.0 accelerate==0.34.2 datasets==3.1.0 evaluate==0.4.3 transformers[sentencepiece]==4.44.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCAhZukN3nGh"
      },
      "source": [
        "Each model on HuggingFace has it’s own model card, a descriptive accounting of various metadata\n",
        "about the model, such as it’s training data and in what applications the model is used for. Take\n",
        "note of the [Limitations and bias section](https://huggingface.co/course/chapter1/8?fw=pt) of the model card for the BERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYsGjGTq3mwV",
        "outputId": "8c5e5723-1e03-48ab-eea4-e34ef63c97c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['carpenter', 'farmer', 'baker', 'tailor', 'salesman']\n",
            "['nurse', 'waitress', 'teacher', 'prostitute', 'maid']\n"
          ]
        }
      ],
      "source": [
        "############ Bias of BERT (DO NOT MODIFY) ############\n",
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "result = unmasker(\"The man works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"The woman works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD1RCh_B9zxX"
      },
      "source": [
        "$\\textbf{Question 3.1}$ (8 points)\n",
        "\n",
        "Play around with the above set of two entries given to the `unmasker`. Make two\n",
        "original sets, of at least size two, of fill-mask prompts that induce the model to exhibit\n",
        "negative bias towards a traditionally minoritized population (such as women, people of color,\n",
        "queer, lower caste, etc.) and a positive bias towards a traditionally normative population\n",
        "(men, white, straight, upper caste, etc.). In your report, define your assumptions/context of\n",
        "what is “minoritized” and what is “normative”. What biases are your examples showing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER QUESTION 3.1**\n",
        "\n",
        "Let's begin by rephrasing this question so a non-initiated person could understand : produce pairs of sentences that \"trick\" a model into demonstrating its biases in *two directions* (favors normativsm, pejorative against non-normative). This is done by the model returning the 5 tokens it deems most probable given an input sentence with a masked word, where the returned tokens are predictions of the masked word in that sentence.\n",
        "\n",
        "Now, how come models tend to exhibit biases in their behaviour? Are they not supposed to be *objective* mathematical tools, free from the irrational biases steming from humans bigotry? The answer is a bit disheartening : these models indeed are *objective mathematical tools*, and it is a matter of what they are trained on. Said directly, the positive biases towards the normative and its inverse for the non-normative is inherent to the training datasets used with models. Since what they are trained on contains these biases, it is only natural that it learns to fit and replicate them. To put it in the words of my previous NLP teacher Fatiha Sadat, `garbage in, garbage out` (she does interesting work at the intersection of NLP and AI-for-good).\n",
        "\n",
        "Let's now clarify a bit what is meant by *normative* and *minoritized* and how it relates to data these models are trained on, notions on which computer science students don't get much time to dwell on (I assume it must be the same the business students of HEC). To put it very simply at first, *normativeness* is the tendency to judge things based on an individual impression of what is *normal*, what the *norm* is. How people come to define certain things as normal and others not is a process highly influenced by the social constructs of an individual (which significantly varies accross places on Earth and times in History). In opposite of the *normative*, you have everything that deviates from the norm, or more precisely, anything that a given individual *interprets as deviating from the norm*. We can be put these in the category of *minoritized* and *marginalized*. A first sophism that must be discarded is the idea that yes, it is better to marginalize some behaviours that we do not want in society (like serious crimes), but that would be extremely besides the point. What is important here is all the subtle (and not so-subtle) ways which pejorative and irrational opinions about what deviates from the norm seeps through every aspect of human society, and here namely, in **produced text**. For a good part of history, written text was a priviledge for the few and the ruling class, usually reflecting their own values and definitions of what was *good*, *desired* or *normal* behaviour. This is reflected on the data LLMs are trained on : books written by priviledged white men or discourse from societies from Western WASP societies (White Anglo-Saxon Protestant). It also shows in more subtle ways, like our biases related to sex, gender, and class, amongst others, when describing or talking about various things in life. \n",
        "\n",
        "The key idea therefore is that if no effort is made in selecting quality training data, the model will replicate what it sees in the data. I must admit, I am not the most well-versed in the research on this topic : I know there is an **important** and ever-increasing body of research on techniques for allieviating these issues without having to worry too much about the quality of the data. Considering the set up of BERT, which is trained on BooksCorpus and English Wikipedia, it is only normal that it exhibits such biases. All this models done is find what is the most probable token, *given its training data*.\n",
        "\n",
        "To demonstrate these, I wanted to show biases associated to gender and to socio-cultural determinants, like class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO6Rf8ax-AUB",
        "outputId": "343ac670-c17b-4fa0-ce01-a1adf1856e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['process', 'approach', 'system', 'model', 'theory']\n",
            "['one', 'man', 'ones', 'people', 'woman']\n"
          ]
        }
      ],
      "source": [
        "##### Bias Section ####\n",
        "\n",
        "# Negative-Bias Towards Minoritized, Positive-Bias Towards Normative\n",
        "# Case 1 to implement\n",
        "result = unmasker(\"The man was promoted as [MASK].\") # the normative\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"The woman was promoted as [MASK].\") # the minoritized\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"Out of my two parents, the richest is [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"This brilliant scientist was a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"The man was a brilliant scientist in [MASK].\") # the normative\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"The woman was a brilliant scientist in [MASK].\") # the minoritized\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "# Case 2 to implement\n",
        "result = unmasker(\"The rich are usually [MASK].\") # the normative\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"The poor are usually [MASK].\") # the minoritized\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"Criminals usually are [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"People founds in slums are [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86QSJqFg-eOa"
      },
      "source": [
        "$\\textbf{Question 3.2}$ (2 points)\n",
        "Come up with one “switched” (anti-stereotype) example where the model exhibits\n",
        "positive-negative bias the other way around. Explain the bias being shown here, how you\n",
        "came up with this example, and include this example in the notebook submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER QUESTION 3.2**\n",
        "\n",
        "Close to my experience is how fit a parent of a certain gender is for child rearing. We usually see women as more fit for parenting than men. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF9_STdITtnK"
      },
      "outputs": [],
      "source": [
        "#### Switched Example Section ####\n",
        "\n",
        "result = unmasker(\"A child raised by a single mom is [MASK].\")\n",
        "print([r[\"token_str\"] for r in result]) # the minoritized\n",
        "\n",
        "result = unmasker(\"A child raised by a singled dad is [MASK].\") # the normative\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"Women usually make for [MASK] parents.\")\n",
        "print([r[\"token_str\"] for r in result]) # the minoritized\n",
        "\n",
        "result = unmasker(\"Men usually make for [MASK] parents.\") # the normative\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "homeWorkEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
