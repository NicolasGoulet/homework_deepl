{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtS0KIifreXc"
      },
      "source": [
        "# Machine Learning II: Deep Learning and Applications\n",
        "# Homework 2\n",
        "\n",
        "**Due date: Nov 3th, 2025**\n",
        "\n",
        "### Instructions\n",
        "- Make a copy of this notebook in your own Colab and complete the questions there.\n",
        "- You can add more cells if necessary. You may also add descriptions to your code, though it is not mandatory.\n",
        "- Make sure the notebook can run through by *Runtime -> Run all*. **Keep all cell outputs** for grading.\n",
        "- Submit the link of your notebook [here](https://docs.google.com/forms/d/e/1FAIpQLSdEhoIthUqZpgA6WmsS-hUFPZebU4CgtPMMno2Bnm4AduYKcw/viewform?usp=sharing&ouid=108990008229336794809). Please **add TAs as editors** (below) so that you can receive feedback from TAs.\n",
        "  - Click `Share` and add zhihao.zhan@mila.quebec and xinyu.yuan@mila.quebec as editors before your submission.\n",
        "\n",
        "### Note\n",
        "A friendly reminder from the TAs: These exercises are fundamental, so we strongly encourage you to complete them with little to no assistance from ChatGPT, especially if you're pursuing a career as an MLE or applied scientist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q23qmcGYr6AG"
      },
      "source": [
        "# Environment Setup\n",
        "Install necessary python packages for this homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_OZwXO-r5OB",
        "outputId": "614adde6-32d2-4311-cd5a-81e16437de03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1+cu118) (3.1.4)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1+cu118)\n",
            "  Using cached https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.30.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (15.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
            "Installing collected packages: triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0\n",
            "    Uninstalling torch-2.5.0:\n",
            "      Successfully uninstalled torch-2.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.0.1+cu118 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.1+cu118 triton-2.0.0\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install datasets transformers\n",
        "!pip install tiktoken\n",
        "!pip install omegaconf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7a-_yxpWV2T",
        "outputId": "dfa63d56-2bb2-41f5-e47f-16158b4a766e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A29mp7wsC_I1"
      },
      "source": [
        "Now let's download all the files needed using the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydUVO0yBDCO_",
        "outputId": "c6e5f00b-cfc6-4fbc-95f5-7c61299ecc95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17XsqhDy_GCjJex7Ekg6PKYEZUKkL_iqH\n",
            "To: /home/alkan/homework_deepl/english-tokenizer.json\n",
            "100%|██████████████████████████████████████| 32.7k/32.7k [00:00<00:00, 2.29MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1GSTq3NQO519BkEG9Bid7003Awh17YrXx\n",
            "From (redirected): https://drive.google.com/uc?id=1GSTq3NQO519BkEG9Bid7003Awh17YrXx&confirm=t&uuid=637ada33-36c3-440f-94b5-08e6497d808f\n",
            "To: /home/alkan/homework_deepl/fixed_initialized_model.pt\n",
            "100%|████████████████████████████████████████| 116M/116M [00:14<00:00, 8.11MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=18LcbPdiyaWAdnBUfwSSx6lmzVE0cSWrP\n",
            "From (redirected): https://drive.google.com/uc?id=18LcbPdiyaWAdnBUfwSSx6lmzVE0cSWrP&confirm=t&uuid=444d1a67-3a6c-4982-a030-86a6a56598ea\n",
            "To: /home/alkan/homework_deepl/tokens.npz\n",
            "100%|████████████████████████████████████████| 570M/570M [01:17<00:00, 7.38MB/s]\n"
          ]
        }
      ],
      "source": [
        "! gdown \"https://drive.google.com/uc?id=17XsqhDy_GCjJex7Ekg6PKYEZUKkL_iqH\"\n",
        "! gdown \"https://drive.google.com/uc?id=1GSTq3NQO519BkEG9Bid7003Awh17YrXx\"\n",
        "! gdown \"https://drive.google.com/uc?id=18LcbPdiyaWAdnBUfwSSx6lmzVE0cSWrP\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzvp3rx9sMvj"
      },
      "source": [
        "# Task 1: Transformer pre-training pipeline using HuggingFace library\n",
        "\n",
        "\n",
        " In this task, you will develop a basic Transformer model and explore training processes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmE0HY0gLwzV"
      },
      "source": [
        "We first compile some utilization functions.\n",
        "\n",
        "**Note: this part of function has to be compiled to run following section. Do not modify the seed to avoid incorrect evaluation results.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Z7Ig6J4LwM-"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------------ #\n",
        "###############  Utilization Functions (DO NOT MODIFY) ###############\n",
        "\n",
        "import torch\n",
        "import io\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def determine_device() -> str:\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    return \"cpu\"\n",
        "\n",
        "\n",
        "def estimate_model_disk_size(model: torch.nn.Module) -> int:\n",
        "    with io.BytesIO() as byte_stream:\n",
        "        torch.save(model.state_dict(), byte_stream)\n",
        "        return byte_stream.tell()\n",
        "\n",
        "def count_params(model: torch.nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "def enable_tf32() -> None:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "def set_seed(seed):\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "# ------------------------------------------------------------------------------------ #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsia4UuvsgTS"
      },
      "source": [
        "## Transformer Implementation\n",
        "\n",
        "In this section, you will implement the key components of the Transformer architecture, specifically the Transformer decoder as depicted in the following Figure 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovy3hlz6pqQ8"
      },
      "source": [
        "![image](https://drive.google.com/uc?id=1Ia7d-1_hdk31E9BV-4_3PYgmNX3YgbM5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvVEk3Z4oWB4"
      },
      "source": [
        "You will begin by creating a decoder-only transformer model, following the provided code structure in the following `Model` section 1 and 2. This structure includes all necessary classes and function declarations required for your submission. Specifically, code to implement is occupied with \"...\" with clear commented note \"[TODO]\", and is detailed in the following subquestions. Please,\n",
        "\n",
        "1. **avoid altering these elements or adding new Python dependencies** to ensure compatability with the automated testing pipeline, which cold otherwise lead to test failures.\n",
        "\n",
        "2. **aim for a model that is efficiently implemented**, favoring the use of PyTorch functions and avoiding loop-based matrix operations\n",
        "\n",
        "3. **You should not use overly simplistic layers or functions** like torch.nn.TransformerDecoder.\n",
        "\n",
        "If in doubt about the appropriateness of a method, consult with the course TAs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sZcEKjnebrf6"
      },
      "outputs": [],
      "source": [
        "############ Model Section 1 (DO NOT MODIFY) ############\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Dimension symbols:\n",
        "    B - batch size\n",
        "    S - sequence length\n",
        "    D - hidden dimension (n_embd)\n",
        "    H - number of attention heads (n_head)\n",
        "    HD - hidden dimension of a single attention head (d // n_head)\n",
        "    V - size of the vocabulary\n",
        "\"\"\"\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, n_embd, head_size, dropout, n_positions):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(n_positions, n_positions)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        #Note: this dropout randomly prevents some tokens from communicating with each other\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x) #shape (B,T, head_size)\n",
        "        q = self.query(x) #shape (B,T, head_size)\n",
        "        v = self.value(x) #shape (B,T, head_size)\n",
        "\n",
        "        #compute self-attention scores\n",
        "        wei = q @ k.transpose(-2, -1) #shape (B,T, head_size) @ (B,head_size,T) --> (B,T,T)\n",
        "        wei *= C**-0.5 #scale by sqrt(d_k) as per paper, so that variance of the wei is 1\n",
        "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B,T,T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        #perform weighted aggregation of values\n",
        "        out = wei @ v # (B, T, T) @ (B, T, head_size) --> (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-head attention as a collection of heads with concatenated outputs.\"\"\"\n",
        "    def __init__(self, n_embd, n_head, dropout, n_positions):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.heads = nn.ModuleList([Head(n_embd, head_size, dropout, n_positions) for _ in range(n_head)])\n",
        "        self.proj  = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" the feed forward network (FFN) in the paper\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd*4, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"A single decoder block in a decoder language model.\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, dropout, n_positions):\n",
        "        \"\"\"Initialize the modules used in a decoder block.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadAttention(n_embd, n_head, dropout, n_positions)\n",
        "        self.mlp = FeedForward(n_embd, dropout)\n",
        "        self.ln_2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(\n",
        "        self, x: torch.FloatTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "        x = self.ln_1(x + self.attn(x))\n",
        "        x = self.ln_2(x + self.mlp(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kn7QRwvQDM8_"
      },
      "outputs": [],
      "source": [
        "############ Model Section 2 ############\n",
        "\n",
        "class DecoderLM(nn.Module):\n",
        "    \"\"\"The decoder language model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab: int,\n",
        "        n_embd: int,\n",
        "        n_head: int,\n",
        "        n_positions: int,\n",
        "        n_layer: int,\n",
        "        p_dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_vocab = n_vocab\n",
        "        self.n_embd = n_embd\n",
        "        self.n_head = n_head\n",
        "        self.n_positions = n_positions\n",
        "        self.n_layer = n_layer\n",
        "        self.p_dropout = p_dropout\n",
        "\n",
        "        self.token_embeddings = nn.Embedding(n_vocab, n_embd)\n",
        "        self.position_embeddings = nn.Embedding(n_positions, n_embd)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [DecoderBlock(n_embd=n_embd, n_head=n_head, dropout=p_dropout, n_positions=n_positions) for _ in range(n_layer)]\n",
        "        )\n",
        "        self.lm_head = nn.Linear(self.n_embd, self.n_vocab, bias=False)\n",
        "        # NOTE: layer_norm should be put after transformer blocks `self.blocks`,\n",
        "        # and before the language model head `self.lm_head`\n",
        "        self.ln = nn.LayerNorm(n_embd)\n",
        "        self.dropout = nn.Dropout(self.p_dropout)\n",
        "\n",
        "        # initialize weights according to nanoGPT\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith(\"out_proj.weight\"):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / torch.sqrt(torch.tensor(2 * n_layer)))\n",
        "\n",
        "        # tie the output projection weights to the token embedding weights\n",
        "        self.lm_head.weight = self.token_embeddings.weight\n",
        "\n",
        "        # count flops per token\n",
        "        self.flops_per_token = (\n",
        "            6 * count_params(self) + 12 * n_layer * n_embd * n_positions\n",
        "        )\n",
        "\n",
        "    def embed(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"Convert input_ids to embeddings (token_embeddings + positional_embeddings).\n",
        "\n",
        "        Args:\n",
        "            input_ids: tokens ids with shape (B x S)\n",
        "\n",
        "        Returns:\n",
        "            embeddings: token representations with shape (B x S x D)\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Position ids are indices of tokens in the sequence. They are simply [0, 1, 2, ...] for every sequence in the\n",
        "        batch.\n",
        "\n",
        "        Example (B = 2, S = 5):\n",
        "\n",
        "        position_ids = tensor([\n",
        "         [0, 1, 2, 3, 4],\n",
        "         [0, 1, 2, 3, 4]\n",
        "        ])\n",
        "        \"\"\"\n",
        "\n",
        "        assert input_ids.shape[1] <= self.n_positions\n",
        "        # B = batch_size, S = seq_len\n",
        "        B, S = input_ids.shape\n",
        "\n",
        "        # token embeddings: (B, S, D)\n",
        "        token_embeddings = self.token_embeddings(input_ids)\n",
        "\n",
        "        # position ids: (1, S) -> (B, S)\n",
        "        position_ids = torch.arange(S, device=input_ids.device).unsqueeze(0).expand(B, S)\n",
        "\n",
        "        # positional embeddings: (B, S, D)\n",
        "        positional_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        # sum + dropout\n",
        "        return self.dropout(token_embeddings + positional_embeddings)\n",
        "\n",
        "    def token_logits(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"Project the final hidden states of the model to token logits.\n",
        "\n",
        "        Args:\n",
        "            x: hidden states produced by the final decoder block (B x S x D)\n",
        "\n",
        "        Returns:\n",
        "            logits: logits corresponding to the predicted next token likelihoods (B x S x V)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"A forward pass of the decoder LM, converting input_ids to token logits.\n",
        "\n",
        "        Args:\n",
        "            input_ids: tokens ids with shape (B x S)\n",
        "\n",
        "        Returns:\n",
        "            logits: logits corresponding to the predicted next token likelihoods (B x S x V)\n",
        "        \"\"\"\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"A forward pass of the decoder LM, converting input_ids to token logits.\"\"\"\n",
        "        # 1) embed tokens + positions\n",
        "        x = self.embed(input_ids)          # (B, S, D)\n",
        "\n",
        "        # 2) pass through decoder blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)                   # (B, S, D)\n",
        "\n",
        "        # 3) final layer norm\n",
        "        x = self.ln(x)                     # (B, S, D)\n",
        "\n",
        "        # 4) project to vocab\n",
        "        logits = self.token_logits(x)      # (B, S, V)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "414k1aqv_33C"
      },
      "source": [
        "$\\textbf{Question 1.1}$ (5 points) **Weight Tying**\n",
        "\n",
        "[Press and Wolf (2017)](https://aclanthology.org/E17-2025/) proposed a weight tying technique for projecting hidden states of a language model to token logits.\n",
        "\n",
        "**Read** this paper first.\n",
        "\n",
        "**Write code** in `DecoderLM.__init__()` to implement the weight Tying technique\n",
        "\n",
        "**In your report**, explain what weight tying does.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER TO QUESTION 1.1**\n",
        "\n",
        "The general idea is to have the mdel use the same matrix to read and predict words. \n",
        "\n",
        "In models like the one considered here, we usually have two big vocab metrics of different dimensions, i.e. the input embedding matrix (which turns tokens into vectors) and the output projection (which turns hidden vectors into logits over vocab). This results in two seperate learnable matrices. \n",
        "\n",
        "Weight tying forces those two matrices to be the same tensor, resulting in one parameter matrix in memory, with both layers pointing to it. This has the following double-advantage : (i) as the model learns better embeddings, it improves at the same time the output head ; (ii) similarly, when the model learns better output predictions, it also improves the embeddings. \n",
        "\n",
        "This is possible due to a key insight that both operations are two side of the same coins : *given a token i, what vector represents it?\" VS \"given a vector, how likely is token i?\". It therefore makes sense to have them share parameters. \n",
        "\n",
        "This resulted in cutting the number of parameters in half WHILE increasing performance. Something that we have seen recently with the Tiny Models by Alexia Jolicoeur-Martineau who achieved better results than gemini models with 0.01% of the parameters and an under 500USD budget. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfLwx9Iwm8dO"
      },
      "source": [
        "$\\textbf{Question 1.2}$ (21 points) **Transformer Model**\n",
        "\n",
        "\n",
        "Recall the Transformer Decoder shown in Figure 1. You are expected\n",
        "to **implement the `DecoderLM` class** for the full decoder model:\n",
        "\n",
        "   - the embedding step (6 points): implement the `DecoderLM.embed()` function, to convert input token indices into token embeddings, combine with positional embeddings to create embeddings for transformer input.\n",
        "   - the final output logits (3 points): implement the `DecoderLM.token_logits()` function, to project the final hidden states of the model to token logits.\n",
        "   - decoder blocks (12 points): implement the `DecoderLM.forward()` function by calling `DecoderLM.embed()`, passing the resulted embeddings to transformer blocks as input, get the final hidden states from the transformer blocks, and calling `DecoderLM.token_logits()` to get the final token logits.\n",
        "\n",
        "Now please read `Model` section 1 carefully and complete the `Model` section 2 above, by implementing all the \"[TODO]\" specified. Your implementation will earn points for each of the \"[TODO]\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BZukjtqslAe"
      },
      "source": [
        "## Transformer Pre-training\n",
        "\n",
        "Now that you've set up the transformer, it's time to begin training the model! For ease of implementation and testing, the tokenized input will be provided to you. We use a portion of the C4 corpus, a refined subset of the Common Crawl web corpus. The dataset is automatically downloaded as a part of the training script, eliminaring the need for you to manually access it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbSVJkHE2_S6"
      },
      "source": [
        "$\\textbf{Question 2.1}$ (24 points) **Model Training Pipeline**\n",
        "\n",
        "The training pipeline is outlined in the `Model Training` section 1, 2, 3, and 4 below. Now please read `Model Training` section 1 and 4 carefully and complete the `Model Training` section 2 and 3 below, by implementing all the \"[TODO]\" specified. Specifically, you are expected to **implement three key functions**:\n",
        "\n",
        "- cosine_lr_schedule (6 points) - a learning rate scheduler that uses Cosine annealing (refer to question 3.2).\n",
        "- compute_language_modeling_loss (6 points) - the function to calculate loss for both training and evaluating the model.\n",
        "- train (12 points) - the main loop for training the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayRzdP7v1kYj"
      },
      "source": [
        "We use [wandb](https://wandb.ai/) for logging the training curves for visualization. you will need to have a [weights & biases account](https://wandb.ai/login)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VoYsADP1lLN",
        "outputId": "9d689541-3dc8-48be-dbc9-52344713c6c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "Aborted!\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! wandb login --relogin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/alkan/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnicolas-goulet\u001b[0m (\u001b[33mnicolas-goulet-hec\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"3fffd4c6035fdc6b19fb004e953a0a4281ffdcb8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfWR5wnCFDB3"
      },
      "source": [
        "**Remember** to consider various training hyperparameters such as batch size, learning rate (including its scheduler), and gradient accumulation, among others. These parameters are set in a configuration dictionary `hyperparam_config` in the `Training Hyper-parameter` section, and we have provided sample configurations for you to adjust these settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YH0Ema1FKLln"
      },
      "outputs": [],
      "source": [
        "############ Training Hyper-parameter Section ############\n",
        "hyperparam_config = {\n",
        "    \"output_dir\": \"outputs/GPT-4060ti-320d\",\n",
        "    \"tokenizer_encoding\": \"gpt2\",\n",
        "    \"model_config\": {\n",
        "        \"n_embd\": 288,      # bigger model\n",
        "        \"n_head\": 8,\n",
        "        \"n_positions\": 256, # longer context → more VRAM\n",
        "        \"n_layer\": 6,\n",
        "    },\n",
        "    \"device\": \"auto\",\n",
        "    \"batch_size\": 48,       # start with 48; if OOM, go 40 or 32\n",
        "    \"seq_len\": 256,\n",
        "    \"num_warmup_steps\": 100,\n",
        "    \"num_training_steps\": 2000,\n",
        "    \"grad_accumulation_steps\": 1,\n",
        "    \"min_lr\": 3e-5,\n",
        "    \"max_lr\": 3e-4,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YwHlkUX6Dbup"
      },
      "outputs": [],
      "source": [
        "############ Model Training Section 1 (DO NOT MODIFY) ############\n",
        "\n",
        "\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from collections import deque\n",
        "from collections.abc import Iterator\n",
        "from contextlib import nullcontext\n",
        "from typing import Callable\n",
        "from rich import print\n",
        "\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import wandb\n",
        "from einops import rearrange\n",
        "from omegaconf import OmegaConf\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "\n",
        "def random_batch_sampler(\n",
        "    tokens: torch.LongTensor, device: str, batch_size: int, seq_len: int\n",
        ") -> Iterator[torch.LongTensor]:\n",
        "    \"\"\"An infinite generator that samples batches of sequences from the tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens: a 1d torch tensor of token ids\n",
        "        device: the device to put the batch on\n",
        "        batch_size: the batch size of the output tensor (B)\n",
        "        seq_len: the sequence length of the output tensor (S)\n",
        "\n",
        "    Returns:\n",
        "        An infinite generator that samples batches of sequences from the\n",
        "        tokens. Each batch has shape (B x S). Every sequence in the batch is\n",
        "        a contiguous subsequence of tokens, sampled uniformly at random. The\n",
        "        output tensor should be on the right device.\n",
        "    \"\"\"\n",
        "\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    while True:\n",
        "        # Generate random starting indices for each sequence in the batch\n",
        "        indices = np.random.randint(0, num_tokens - seq_len + 1, batch_size)\n",
        "\n",
        "        # Gather sequences from the tokens\n",
        "        batch = torch.stack([tokens[i:i + seq_len] for i in indices])\n",
        "\n",
        "        # Move the batch to the specified device\n",
        "        yield batch.to(device)\n",
        "\n",
        "\n",
        "def sequential_batch_sampler(\n",
        "    tokens: torch.LongTensor, device: str, batch_size: int, seq_len: int\n",
        ") -> Iterator[torch.LongTensor]:\n",
        "    \"\"\"A generator that yields batches of tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens: a 1d torch tensor of token ids\n",
        "        device: the device to put the batch on\n",
        "        batch_size: the batch size of the output tensor (B)\n",
        "        seq_len: the sequence length of the output tensor (S)\n",
        "\n",
        "    Returns:\n",
        "        A generator that yields a batch of tokens at a time. Each batch has\n",
        "        shape (B x S). Every sequence in the batch is a contiguous subsequence\n",
        "        of tokens in sequential order. The output tensor should be on the right\n",
        "        device.\n",
        "\n",
        "    Note: If the last batch is incomplete, which could happen when the number\n",
        "        of tokens is not divisible by (batch_size * seq_len), you could drop\n",
        "        the last batch.\n",
        "    \"\"\"\n",
        "\n",
        "    num_tokens = len(tokens)\n",
        "    total_batches = num_tokens // (batch_size * seq_len)\n",
        "\n",
        "    for batch_idx in range(total_batches):\n",
        "        start_idx = batch_idx * batch_size * seq_len\n",
        "        end_idx = start_idx + batch_size * seq_len\n",
        "\n",
        "        # Extract the batch of tokens\n",
        "        batch = tokens[start_idx: end_idx].view(batch_size, seq_len)\n",
        "\n",
        "        # Move the batch to the specified device and yield\n",
        "        yield batch.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yz01X_AHhOIs"
      },
      "outputs": [],
      "source": [
        "############ Model Training Section 2 ############\n",
        "\n",
        "def cosine_lr_schedule(\n",
        "    num_warmup_steps: int,\n",
        "    num_training_steps: int,\n",
        "    min_lr: float,\n",
        "    max_lr: float,\n",
        ") -> Callable[[int], float]:\n",
        "    def get_lr(t: int) -> float:\n",
        "        \"\"\"Outputs the learning rate at step t under the cosine schedule.\n",
        "\n",
        "        Args:\n",
        "            t: the current step number\n",
        "\n",
        "        Returns:\n",
        "            lr: learning rate at step t\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        assert max_lr >= min_lr >= 0.0\n",
        "        assert num_training_steps >= num_warmup_steps >= 0\n",
        "\n",
        "        if t <= num_warmup_steps:\n",
        "            # Linear warmup starting from 0 to max_lr\n",
        "            warmup_steps = max(1, num_warmup_steps)\n",
        "            lr = max_lr * (t / warmup_steps)\n",
        "        elif t >= num_training_steps:\n",
        "            # After training steps, return min_lr\n",
        "            lr = min_lr\n",
        "        else:\n",
        "            # Cosine decay from max_lr to min_lr\n",
        "            # progress goes from 0 → 1 over the decay phase\n",
        "            progress = (t - num_warmup_steps) / (num_training_steps - num_warmup_steps)\n",
        "            cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "            lr = min_lr + (max_lr - min_lr) * cosine\n",
        "        return lr\n",
        "\n",
        "    return get_lr\n",
        "\n",
        "\n",
        "def set_lr(optimizer: torch.optim.Optimizer, lr: float) -> None:\n",
        "    for g in optimizer.param_groups:\n",
        "        g[\"lr\"] = lr\n",
        "\n",
        "\n",
        "\n",
        "def compute_language_modeling_loss(\n",
        "    input_ids: torch.LongTensor, logits: torch.FloatTensor\n",
        ") -> torch.FloatTensor:\n",
        "    \"\"\"Outputs the language modeling loss given input_ids and logits\n",
        "\n",
        "    Args:\n",
        "        input_ids: the input token ids (B, S)\n",
        "        logits: the next token logits produced by the language model (B, S, V)\n",
        "\n",
        "    Returns:\n",
        "        loss: the mean cross entropy loss for next token prediction\n",
        "    \"\"\"\n",
        "    # shift inputs to get labels: each position predicts the NEXT token\n",
        "    # labels: (B, S-1)\n",
        "    labels = input_ids[:, 1:]\n",
        "\n",
        "    # drop the last time step in logits so shapes match: (B, S-1, V)\n",
        "    logits = logits[:, :-1, :]\n",
        "\n",
        "    # flatten for cross-entropy: logits -> (B*(S-1), V), labels -> (B*(S-1),)\n",
        "    B, S_minus_1, V = logits.shape\n",
        "    logits_flat = logits.reshape(B * S_minus_1, V)\n",
        "    labels_flat = labels.reshape(B * S_minus_1)\n",
        "\n",
        "    loss = F.cross_entropy(logits_flat, labels_flat)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7e7Yong_0W-5"
      },
      "outputs": [],
      "source": [
        "############ Model Training Section 3 ############\n",
        "\n",
        "def train(\n",
        "    model: DecoderLM,\n",
        "    batch_sampler: Iterator[torch.LongTensor],\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    lr_schedule: Callable[[int], float],\n",
        "    autocast: torch.autocast | nullcontext = nullcontext(),\n",
        "    num_training_steps: int = 0,\n",
        "    grad_accumulation_steps: int = 1,\n",
        ") -> None:\n",
        "    \"\"\"A training loop for the language model\n",
        "\n",
        "    Args:\n",
        "        model: the decoder LM\n",
        "        batch_sampler: a generator that produces batches of token ids\n",
        "        optimizer: an optimizer for gradient update\n",
        "        lr_schedule: a callable that produces the learning at a step number\n",
        "        autocast: a context manager that handles tensor casting (you do not need\n",
        "          to care about this for your implementation)\n",
        "        num_training_steps: number of steps to train for\n",
        "        grad_accumulation_steps: number of \"micro\" training steps before each\n",
        "          gradient update\n",
        "    \"\"\"\n",
        "    # stores training losses for the 20 latest steps\n",
        "    losses = deque(maxlen=20 * grad_accumulation_steps)\n",
        "\n",
        "    for step in (pbar := trange(num_training_steps)):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # 1) set LR for this step\n",
        "        lr = lr_schedule(step)\n",
        "        set_lr(optimizer, lr)\n",
        "\n",
        "        # 2) accumulate gradients over micro-steps\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        for _ in range(grad_accumulation_steps):\n",
        "            input_ids = next(batch_sampler)\n",
        "            with autocast:\n",
        "                logits = model(input_ids)\n",
        "            loss = compute_language_modeling_loss(input_ids, logits)\n",
        "            # scale loss so total gradient over accumulation is same as single step\n",
        "            (loss / grad_accumulation_steps).backward()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # 3) update params\n",
        "        optimizer.step()\n",
        "\n",
        "        # logging stuff\n",
        "        loss_mean = np.mean(losses).item()\n",
        "\n",
        "        FLOPs_per_step = (\n",
        "            model.flops_per_token\n",
        "            * input_ids.shape[0]\n",
        "            * input_ids.shape[1]\n",
        "            * grad_accumulation_steps\n",
        "        )\n",
        "        t1 = time.time()\n",
        "        dt = t1 - t0\n",
        "        t0 = t1\n",
        "        pbar.set_postfix(\n",
        "            {\n",
        "                \"train loss\": f\"{loss_mean:.2f}\",\n",
        "                \"TFLOPS\": f\"{FLOPs_per_step / dt / 1e12:.1f}\",\n",
        "            }\n",
        "        )\n",
        "        wandb.log({\"train-loss\": loss_mean, \"learning-rate\": lr}, step=step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgi3T6IhCdEA"
      },
      "source": [
        "$\\textbf{Question 2.2}$ (10 points) **Model Training Results**\n",
        "\n",
        "**Execute the `Model Training` Section 4** below to run the pre-training pipeline, and report the training loss curve. (Note: runnning for 60 steps takes around **8 minutes** for training.)\n",
        "\n",
        "\n",
        "**In your report**, answer the following:\n",
        "\n",
        "A. Plot the training curve for the default `hyperparam_config` (use a screenshot from weights & biases: https://wandb.ai/site), and discuss how and why it looks like that.\n",
        "\n",
        "B. Discuss your `DecoderLM` pre-training experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER QUESTION 2.2**\n",
        "\n",
        "A. TODO WHY IT LOOKS LIKE THAT\n",
        "B. DISCUSS DECODERLM PRETRAINING EXPERIMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M0iMgp89hQWb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/alkan/VirtualEnv/homeWorkEnv/lib/python3.10/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">########################################\n",
              "output_dir: outputs/GPT-4060ti-320d\n",
              "tokenizer_encoding: gpt2\n",
              "model_config:\n",
              "  n_embd: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">288</span>\n",
              "  n_head: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>\n",
              "  n_positions: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>\n",
              "  n_layer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>\n",
              "device: auto\n",
              "batch_size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48</span>\n",
              "seq_len: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span>\n",
              "num_warmup_steps: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>\n",
              "num_training_steps: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2000</span>\n",
              "grad_accumulation_steps: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "min_lr: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0e-05</span>\n",
              "max_lr: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0003</span>\n",
              "########################################\n",
              "</pre>\n"
            ],
            "text/plain": [
              "########################################\n",
              "output_dir: outputs/GPT-4060ti-320d\n",
              "tokenizer_encoding: gpt2\n",
              "model_config:\n",
              "  n_embd: \u001b[1;36m288\u001b[0m\n",
              "  n_head: \u001b[1;36m8\u001b[0m\n",
              "  n_positions: \u001b[1;36m256\u001b[0m\n",
              "  n_layer: \u001b[1;36m6\u001b[0m\n",
              "device: auto\n",
              "batch_size: \u001b[1;36m48\u001b[0m\n",
              "seq_len: \u001b[1;36m256\u001b[0m\n",
              "num_warmup_steps: \u001b[1;36m100\u001b[0m\n",
              "num_training_steps: \u001b[1;36m2000\u001b[0m\n",
              "grad_accumulation_steps: \u001b[1;36m1\u001b[0m\n",
              "min_lr: \u001b[1;36m3.0e-05\u001b[0m\n",
              "max_lr: \u001b[1;36m0.0003\u001b[0m\n",
              "########################################\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/alkan/homework_deepl/wandb/run-20251030_211803-xqqg626f</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nicolas-goulet-hec/llms-hw2/runs/xqqg626f' target=\"_blank\">silent-enchantment-4</a></strong> to <a href='https://wandb.ai/nicolas-goulet-hec/llms-hw2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nicolas-goulet-hec/llms-hw2' target=\"_blank\">https://wandb.ai/nicolas-goulet-hec/llms-hw2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nicolas-goulet-hec/llms-hw2/runs/xqqg626f' target=\"_blank\">https://wandb.ai/nicolas-goulet-hec/llms-hw2/runs/xqqg626f</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model parameters = 21M\n",
              "</pre>\n"
            ],
            "text/plain": [
              "model parameters = 21M\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your model is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">94.</span>8MB. This should be within the 100MB limit of Gradescope.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Your model is \u001b[1;36m94.\u001b[0m8MB. This should be within the 100MB limit of Gradescope.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">train dataset tokens = 520M\n",
              "</pre>\n"
            ],
            "text/plain": [
              "train dataset tokens = 520M\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">train FLOPs = <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.16e+15</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "train FLOPs = \u001b[1;36m3.16e+15\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [06:49<00:00,  4.88it/s, train loss=5.82, TFLOPS=7.5]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model saved to outputs/GPT-4060ti-320d/model.pt\n",
              "</pre>\n"
            ],
            "text/plain": [
              "model saved to outputs/GPT-4060ti-320d/model.pt\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "############ Model Training Section 4 (DO NOT MODIFY) ############\n",
        "\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate(\n",
        "    model: DecoderLM,\n",
        "    batch_sampler: Iterator[torch.LongTensor],\n",
        "    autocast: torch.autocast | nullcontext = nullcontext(),\n",
        ") -> dict[str, float]:\n",
        "    losses = []\n",
        "\n",
        "    for input_ids in tqdm(batch_sampler, desc=\"evaluating..\"):\n",
        "        with autocast:\n",
        "            logits = model(input_ids)\n",
        "        loss = compute_language_modeling_loss(input_ids, logits)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # mean of the losses is the average negative log likelihood\n",
        "    mean_loss = sum(losses) / len(losses)\n",
        "    perplexity = math.exp(mean_loss)\n",
        "\n",
        "    eval_results = {\n",
        "        \"val-loss\": mean_loss,\n",
        "        \"val-perplexity\": perplexity,\n",
        "    }\n",
        "    wandb.log(eval_results)\n",
        "    return eval_results\n",
        "\n",
        "def main():\n",
        "    enable_tf32()\n",
        "\n",
        "    # create an output directory and dump the configuration file\n",
        "    config = OmegaConf.create(hyperparam_config)\n",
        "    os.makedirs(config.output_dir, exist_ok=True)\n",
        "    OmegaConf.save(config, os.path.join(config.output_dir, \"config.yaml\"))\n",
        "    print(\"#\" * 40, OmegaConf.to_yaml(config).strip(), \"#\" * 40, sep=\"\\n\")\n",
        "    wandb.init(project=\"llms-hw2\", config=OmegaConf.to_container(config))\n",
        "\n",
        "    # initialize tokenizer and model\n",
        "    tokenizer = tiktoken.get_encoding(config.tokenizer_encoding)\n",
        "    device = determine_device() if config.device == \"auto\" else config.device\n",
        "    model = DecoderLM(tokenizer.n_vocab, **config.model_config).to(device)\n",
        "    print(f\"model parameters = {count_params(model) / 1e6:.0f}M\")\n",
        "\n",
        "    model_disk_size_MB = estimate_model_disk_size(model) * 1e-6\n",
        "    if model_disk_size_MB > 98:\n",
        "        print(\n",
        "            f\"[red]WARNING: your model is {model_disk_size_MB:.1f}MB. \"\n",
        "            \"The largest model size allowed by GradeScope is 100MB, \"\n",
        "            \"and you may have trouble with submitting the assignment. \"\n",
        "            \"Please update your config so your model is at most 100 MB.[/red]\"\n",
        "        )\n",
        "    else:\n",
        "        print(\n",
        "            f\"Your model is {model_disk_size_MB:.1f}MB. This should be within \"\n",
        "            \"the 100MB limit of Gradescope.\"\n",
        "        )\n",
        "\n",
        "    # prepare data and data generator\n",
        "    assert config.seq_len <= config.model_config.n_positions\n",
        "    tokens = np.load(\"tokens.npz\")\n",
        "\n",
        "    train_tokens = torch.from_numpy(tokens[\"train\"].astype(int))\n",
        "    val_tokens = torch.from_numpy(tokens[\"val\"].astype(int))\n",
        "\n",
        "    train_sampler = random_batch_sampler(\n",
        "        train_tokens, device, config.batch_size, config.seq_len\n",
        "    )\n",
        "    val_sampler = sequential_batch_sampler(\n",
        "        val_tokens, device, config.batch_size, config.seq_len\n",
        "    )\n",
        "    print(f\"train dataset tokens = {len(train_tokens) / 1e6:.0f}M\")\n",
        "    FLOPs = (\n",
        "        model.flops_per_token\n",
        "        * config.num_training_steps\n",
        "        * config.grad_accumulation_steps\n",
        "        * config.batch_size\n",
        "        * config.seq_len\n",
        "    )\n",
        "    print(f\"train FLOPs = {FLOPs:.2e}\")\n",
        "    if FLOPs > 1e17:\n",
        "        print(\n",
        "            f\"[red]WARNING: your train FLOPs is {FLOPs:.2e}. \"\n",
        "            \"This is more than the max compute that we allow (1e+17). \"\n",
        "            \"Please reduce your model size or train steps.[/red]\"\n",
        "        )\n",
        "\n",
        "    # prepare optimizer and lr schedule\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=0.0,  # will set this dynamically in the training loop\n",
        "        betas=(0.9, 0.95),\n",
        "        fused=device == \"cuda\",\n",
        "    )\n",
        "    lr_schedule = cosine_lr_schedule(\n",
        "        config.num_warmup_steps, config.num_training_steps, config.min_lr, config.max_lr\n",
        "    )\n",
        "    autocast = (\n",
        "        torch.autocast(\n",
        "            device,\n",
        "            dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32),\n",
        "        )\n",
        "        if device == \"cuda\"\n",
        "        else nullcontext()\n",
        "    )\n",
        "    # training\n",
        "    model.train()\n",
        "    train(\n",
        "        model,\n",
        "        train_sampler,\n",
        "        optimizer,\n",
        "        lr_schedule,\n",
        "        autocast,\n",
        "        config.num_training_steps,\n",
        "        config.grad_accumulation_steps,\n",
        "    )\n",
        "\n",
        "    # save the trained model\n",
        "    model_path = os.path.join(config.output_dir, \"model.pt\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"model saved to {model_path}\")\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijz5fOtt3nb0"
      },
      "source": [
        "# Task 2: Bias in Large Language Models (LLMs)\n",
        "\n",
        "In this task, you will critically examine limitations and bias of LLMs. You will use a pre-trained BERT model in the following `Bias of BERT` section to analyze the potential sources of bias from the data to the model itself.\n",
        "\n",
        "You need to enter the prompts in this notebook, and **in your discussion**, these prompts need to be detailed and explaned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJUxr5sQWgJu"
      },
      "source": [
        "**NOTE**: Please restart the colab runtime before running codes for Task 2 to avoid package dependency issue. **Go to \"Runtime menu\" and click \"Restart Runtime\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4EOE8jUZ-c7"
      },
      "outputs": [],
      "source": [
        "! pip install -qqq torch==2.5.0 accelerate==0.34.2 datasets==3.1.0 evaluate==0.4.3 transformers[sentencepiece]==4.44.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCAhZukN3nGh"
      },
      "source": [
        "Each model on HuggingFace has it’s own model card, a descriptive accounting of various metadata\n",
        "about the model, such as it’s training data and in what applications the model is used for. Take\n",
        "note of the [Limitations and bias section](https://huggingface.co/course/chapter1/8?fw=pt) of the model card for the BERT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYsGjGTq3mwV",
        "outputId": "8c5e5723-1e03-48ab-eea4-e34ef63c97c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['carpenter', 'farmer', 'baker', 'tailor', 'salesman']\n",
            "['nurse', 'waitress', 'teacher', 'prostitute', 'maid']\n"
          ]
        }
      ],
      "source": [
        "############ Bias of BERT (DO NOT MODIFY) ############\n",
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "result = unmasker(\"The man works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"The woman works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD1RCh_B9zxX"
      },
      "source": [
        "$\\textbf{Question 3.1}$ (8 points)\n",
        "\n",
        "Play around with the above set of two entries given to the `unmasker`. Make two\n",
        "original sets, of at least size two, of fill-mask prompts that induce the model to exhibit\n",
        "negative bias towards a traditionally minoritized population (such as women, people of color,\n",
        "queer, lower caste, etc.) and a positive bias towards a traditionally normative population\n",
        "(men, white, straight, upper caste, etc.). In your report, define your assumptions/context of\n",
        "what is “minoritized” and what is “normative”. What biases are your examples showing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO6Rf8ax-AUB",
        "outputId": "343ac670-c17b-4fa0-ce01-a1adf1856e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['process', 'approach', 'system', 'model', 'theory']\n",
            "['one', 'man', 'ones', 'people', 'woman']\n"
          ]
        }
      ],
      "source": [
        "##### Bias Section ####\n",
        "\n",
        "# Negative-Bias Towards Minoritized, Positive-Bias Towards Normative\n",
        "# Case 1 to implement\n",
        "result = unmasker(\"... [MASK].\") # the normative\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"... [MASK].\") # the minoritized\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "# Case 2 to implement\n",
        "result = unmasker(\"... [MASK].\") # the normative\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"... [MASK].\") # the minoritized\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86QSJqFg-eOa"
      },
      "source": [
        "$\\textbf{Question 3.2}$ (2 points)\n",
        "ome up with one “switched” (anti-stereotype) example where the model exhibits\n",
        "positive-negative bias the other way around. Explain the bias being shown here, how you\n",
        "came up with this example, and include this example in the notebook submission.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF9_STdITtnK"
      },
      "outputs": [],
      "source": [
        "#### Switched Example Section ####\n",
        "\n",
        "result = unmasker(\"... [MASK].\")\n",
        "print([r[\"token_str\"] for r in result]) # the minoritized\n",
        "\n",
        "result = unmasker(\"... [MASK].\") # the normative\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "homeWorkEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
